{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use my GPU\n",
    "# !pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset, Audio, DatasetDict\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2FeatureExtractor, AutoModelForAudioClassification, WhisperForAudioClassification, WhisperProcessor,  TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "from torch.cuda import device_count\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at distil-whisper/distil-large-v3 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21938032f04044138cfbdafdc1d1f791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2329826186e84bc7afe321c443331c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e301a9d117840869aed8f0bf0e158e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored from cffi callback <function SoundFile._init_virtual_io.<locals>.vio_read at 0x000001CE950EADE0>:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\soundfile.py\", line 1246, in vio_read\n",
      "    data_read = file.readinto(buf)\n",
      "                ^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e1604afdbf4ef5a0b84efe19033a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_name = \"distil-whisper/distil-large-v3\" # \"whisper-large-v3_ADReSSO\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_name, num_labels=2, ignore_mismatched_sizes=True\n",
    ")\n",
    "del model.config.__dict__[\"max_length\"]\n",
    "del model.config.__dict__[\"suppress_tokens\"]\n",
    "del model.config.__dict__[\"begin_suppress_tokens\"]\n",
    "model.to(device)\n",
    "model.save_pretrained(\"model\")\n",
    "\n",
    "# model1 = WhisperForAudioClassification.from_pretrained(model_name, num_labels=2)\n",
    "# processor1 = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "preprocess = lambda examples: feature_extractor(\n",
    "    [i[\"array\"][(n := randint(0, len(i[\"array\"]) - (m := min(len(i[\"array\"]), feature_extractor.sampling_rate*30)))) : n + m] for i in examples[\"audio\"]],\n",
    "    sampling_rate=feature_extractor.sampling_rate,\n",
    "    do_normalize=True,\n",
    "    # max_length=16_000*args.sample_duration,\n",
    "    # truncation=True,\n",
    ")\n",
    "\n",
    "#### LOAD DATASET HERE ############\n",
    "AD_PATH = 'ADReSSo21/diagnosis/train/audio/ad'\n",
    "CN_PATH = 'ADReSSo21/diagnosis/train/audio/cn'\n",
    "\n",
    "ad_dataset = (\n",
    "    load_dataset(AD_PATH)\n",
    "    .cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    ")\n",
    "ad_dataset = ad_dataset.map(lambda example: {\"label\": 0})\n",
    "\n",
    "cn_dataset = (\n",
    "    load_dataset(CN_PATH)\n",
    "    .cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    ")\n",
    "cn_dataset = cn_dataset.map(lambda example: {\"label\": 1})\n",
    "\n",
    "dataset = concatenate_datasets([ad_dataset[\"train\"], cn_dataset[\"train\"]])\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset\n",
    "})\n",
    "dataset[\"train\"], dataset[\"valid\"] = dataset[\"train\"].train_test_split(0.25).values()\n",
    "# dataset[\"valid\"], dataset[\"test\"] = dataset[\"valid\"].train_test_split(0.2).values()\n",
    "dataset = dataset.map(preprocess, remove_columns=\"audio\", batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to Tensor (Data must be in PyTorch format, eg: {\"input_values\": audio_tensor, \"label\": label_tensor})\n",
    "# class AudioDataset(Dataset):\n",
    "#     def __init__(self, audio_data, labels):\n",
    "#         self.audio_data = audio_data\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.audio_data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Convert to tensor and return\n",
    "#         audio_tensor = torch.tensor(self.audio_data[idx]).float()  # Make sure it's float\n",
    "#         label_tensor = torch.tensor(self.labels[idx]).long()  # Make sure it's long for classification\n",
    "#         return {\"input_values\": audio_tensor, \"label\": label_tensor}\n",
    "    \n",
    "# train_dataset = {\"input_features\": [torch.tensor(feature, dtype=torch.float) for feature in dataset[\"train\"][\"input_features\"]], \n",
    "#                  \"label\": [torch.tensor(feature, dtype=torch.float) for feature in dataset[\"train\"][\"label\"]]}\n",
    "# valid_dataset = {\"input_features\": [torch.tensor(feature, dtype=torch.float) for feature in dataset[\"valid\"][\"input_features\"]], \n",
    "#                  \"label\": [torch.tensor(feature, dtype=torch.float) for feature in dataset[\"valid\"][\"label\"]]}\n",
    "\n",
    "train_dataset = dataset[\"train\"].with_format(\"torch\")\n",
    "val_dataset = dataset[\"valid\"].with_format(\"torch\")\n",
    "# test_dataset = dataset[\"test\"].with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4315a88c27774739861d3c1f9b1e180e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:599: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7083, 'grad_norm': 3.1401898860931396, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ee5def5d744e45b782143f90c96372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6848700642585754, 'eval_accuracy': 0.5238095238095238, 'eval_f1': 0.0, 'eval_specificity': 1.0, 'eval_runtime': 159.7882, 'eval_samples_per_second': 0.263, 'eval_steps_per_second': 0.131, 'epoch': 0.97}\n",
      "{'loss': 0.6411, 'grad_norm': 3.3013556003570557, 'learning_rate': 2.8888888888888888e-05, 'epoch': 1.29}\n",
      "{'loss': 0.5405, 'grad_norm': 4.796957969665527, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51081a11ae14b689cac5d4524bb1959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5502444505691528, 'eval_accuracy': 0.7142857142857143, 'eval_f1': 0.7391304347826086, 'eval_specificity': 0.5909090909090909, 'eval_runtime': 159.9654, 'eval_samples_per_second': 0.263, 'eval_steps_per_second': 0.131, 'epoch': 2.0}\n",
      "{'loss': 0.2554, 'grad_norm': 2.6861839294433594, 'learning_rate': 2.4444444444444445e-05, 'epoch': 2.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed83cd57a58a430980c8cae628a7fe3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.016992449760437, 'eval_accuracy': 0.6904761904761905, 'eval_f1': 0.7450980392156863, 'eval_specificity': 0.45454545454545453, 'eval_runtime': 174.3948, 'eval_samples_per_second': 0.241, 'eval_steps_per_second': 0.12, 'epoch': 2.97}\n",
      "{'loss': 0.3521, 'grad_norm': 40.06829833984375, 'learning_rate': 2.222222222222222e-05, 'epoch': 3.23}\n",
      "{'loss': 0.5699, 'grad_norm': 13.027278900146484, 'learning_rate': 1.9999999999999998e-05, 'epoch': 3.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba4005119fe49f8814e69ecde444b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5279026031494141, 'eval_accuracy': 0.6904761904761905, 'eval_f1': 0.6666666666666666, 'eval_specificity': 0.7272727272727273, 'eval_runtime': 196.6405, 'eval_samples_per_second': 0.214, 'eval_steps_per_second': 0.107, 'epoch': 4.0}\n",
      "{'loss': 0.0903, 'grad_norm': 57.81535339355469, 'learning_rate': 1.7777777777777777e-05, 'epoch': 4.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdf6f6d2e544b6d9b74c88924eb23a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8971467018127441, 'eval_accuracy': 0.7380952380952381, 'eval_f1': 0.7027027027027027, 'eval_specificity': 0.8181818181818182, 'eval_runtime': 199.6499, 'eval_samples_per_second': 0.21, 'eval_steps_per_second': 0.105, 'epoch': 4.97}\n",
      "{'loss': 0.0963, 'grad_norm': 0.09985890984535217, 'learning_rate': 1.5555555555555555e-05, 'epoch': 5.16}\n",
      "{'loss': 0.0036, 'grad_norm': 0.07194121181964874, 'learning_rate': 1.3333333333333333e-05, 'epoch': 5.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7de54c2d50432da8b5eb3d8209b0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7317893505096436, 'eval_accuracy': 0.8095238095238095, 'eval_f1': 0.8095238095238095, 'eval_specificity': 0.7727272727272727, 'eval_runtime': 198.2276, 'eval_samples_per_second': 0.212, 'eval_steps_per_second': 0.106, 'epoch': 6.0}\n",
      "{'loss': 0.0023, 'grad_norm': 0.047676365822553635, 'learning_rate': 1.111111111111111e-05, 'epoch': 6.45}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca98fadf6a1a468b9967cf4b79cfd350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.071899652481079, 'eval_accuracy': 0.7619047619047619, 'eval_f1': 0.7368421052631579, 'eval_specificity': 0.8181818181818182, 'eval_runtime': 200.0074, 'eval_samples_per_second': 0.21, 'eval_steps_per_second': 0.105, 'epoch': 6.97}\n",
      "{'loss': 0.0017, 'grad_norm': 0.03788022696971893, 'learning_rate': 8.888888888888888e-06, 'epoch': 7.1}\n",
      "{'loss': 0.0014, 'grad_norm': 0.03436962142586708, 'learning_rate': 6.666666666666667e-06, 'epoch': 7.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836cd63383b84c64bec3dc9f1c234c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3117010593414307, 'eval_accuracy': 0.7142857142857143, 'eval_f1': 0.6666666666666666, 'eval_specificity': 0.8181818181818182, 'eval_runtime': 200.2861, 'eval_samples_per_second': 0.21, 'eval_steps_per_second': 0.105, 'epoch': 8.0}\n",
      "{'loss': 0.0013, 'grad_norm': 0.031182970851659775, 'learning_rate': 4.444444444444444e-06, 'epoch': 8.39}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4524188a3b4e67b2051d34842b772a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.361391544342041, 'eval_accuracy': 0.7142857142857143, 'eval_f1': 0.6666666666666666, 'eval_specificity': 0.8181818181818182, 'eval_runtime': 196.6871, 'eval_samples_per_second': 0.214, 'eval_steps_per_second': 0.107, 'epoch': 8.97}\n",
      "{'loss': 0.0012, 'grad_norm': 0.03122571110725403, 'learning_rate': 2.222222222222222e-06, 'epoch': 9.03}\n",
      "{'loss': 0.0012, 'grad_norm': 0.02977282926440239, 'learning_rate': 0.0, 'epoch': 9.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc96d31494c45a39f55fd7185953441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3726603984832764, 'eval_accuracy': 0.7142857142857143, 'eval_f1': 0.6666666666666666, 'eval_specificity': 0.8181818181818182, 'eval_runtime': 198.9486, 'eval_samples_per_second': 0.211, 'eval_steps_per_second': 0.106, 'epoch': 9.68}\n",
      "{'train_runtime': 11724.7202, 'train_samples_per_second': 0.106, 'train_steps_per_second': 0.013, 'train_loss': 0.21776453108216326, 'epoch': 9.68}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.21776453108216326, metrics={'train_runtime': 11724.7202, 'train_samples_per_second': 0.106, 'train_steps_per_second': 0.013, 'total_flos': 1.756691463168e+18, 'train_loss': 0.21776453108216326, 'epoch': 9.67741935483871})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "specificity = evaluate.load(\"nevikw39/specificity\")\n",
    "\n",
    "## training part\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/whisper-large-v3_ADReSSo\" + (\"_fp16\" if False else \"\"),\n",
    "    fp16=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=2, # 8\n",
    "    per_device_eval_batch_size=2,# 8\n",
    "    gradient_accumulation_steps=4,\n",
    "    # gradient_checkpointing=True,\n",
    "    num_train_epochs=10, # 100\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer (\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset, # dataset[\"valid\"],\n",
    "    # eval_dataset=encoded_dataset[\"test\"].select(np.random.choice(71, 42)),\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=lambda eval_pred: (\n",
    "        accuracy.compute(\n",
    "            predictions=(pred := np.argmax(eval_pred.predictions, axis=1)),\n",
    "            references=eval_pred.label_ids,\n",
    "        ) | f1.compute(\n",
    "            predictions=pred,\n",
    "            references=eval_pred.label_ids,\n",
    "        ) | specificity.compute(\n",
    "            predictions=pred,\n",
    "            references=eval_pred.label_ids,\n",
    "        )\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(10)],\n",
    ")\n",
    "\n",
    "#trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"test_dataset\", test_dataset)\n",
    "\n",
    "# eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8609477c9ef84912be9436f9fdd0b1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the evaluation dataset and capture the evaluation results\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print the evaluation metrics\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_result)\n",
      "File \u001b[1;32mc:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\transformers\\trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3869\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\transformers\\trainer.py:4081\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4079\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m   4080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4081\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4083\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n",
      "File \u001b[1;32mc:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\accelerate\\accelerator.py:2600\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[1;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m   2567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   2568\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;124;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[0;32m   2570\u001b[0m \u001b[38;5;124;03m    they can safely be gathered.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2598\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   2599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\accelerate\\utils\\operations.py:412\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    414\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\accelerate\\utils\\operations.py:679\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[1;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m    676\u001b[0m     new_tensor[indices] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[1;32m--> 679\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\accelerate\\utils\\operations.py:127\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[1;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[0;32m    119\u001b[0m         {\n\u001b[0;32m    120\u001b[0m             k: recursively_apply(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m         }\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\accelerate\\utils\\operations.py:659\u001b[0m, in \u001b[0;36mpad_across_processes.<locals>._pad_across_processes\u001b[1;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[1;32m--> 659\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    660\u001b[0m sizes \u001b[38;5;241m=\u001b[39m gather(size)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the evaluation dataset and capture the evaluation results\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation results:\", eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
