{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0114f7bc0edc406c996bf63fc3b5ba6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f4ed82007a4b35931f797b946d9b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7d5b921b354ff89c32e86d46db3306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e065f311a1ea4ac399dfc46a8ace2f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qscre\\miniconda3\\envs\\aml\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_dataset, Audio, DatasetDict\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from random import randint\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load the feature extractor and the saved model from \"models/whisper-large-v3_ADReSSo\"\n",
    "model_name = \"distil-whisper/distil-large-v3\" # Model name\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "# Load the model from the saved directory\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"models/whisper-large-v3_ADReSSo/checkpoint-93\", num_labels=2, ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Preprocessing function for the audio data\n",
    "preprocess = lambda examples: feature_extractor(\n",
    "    [i[\"array\"][(n := randint(0, len(i[\"array\"]) - (m := min(len(i[\"array\"]), feature_extractor.sampling_rate * 30)))) : n + m] for i in examples[\"audio\"]],\n",
    "    sampling_rate=feature_extractor.sampling_rate,\n",
    "    do_normalize=True,\n",
    ")\n",
    "\n",
    "#### LOAD DATASET HERE ############\n",
    "AD_PATH = 'ADReSSo21/diagnosis/train/audio/ad'\n",
    "CN_PATH = 'ADReSSo21/diagnosis/train/audio/cn'\n",
    "\n",
    "ad_dataset = (\n",
    "    load_dataset(AD_PATH)\n",
    "    .cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    ")\n",
    "ad_dataset = ad_dataset.map(lambda example: {\"label\": 0})\n",
    "\n",
    "cn_dataset = (\n",
    "    load_dataset(CN_PATH)\n",
    "    .cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    ")\n",
    "cn_dataset = cn_dataset.map(lambda example: {\"label\": 1})\n",
    "\n",
    "dataset = concatenate_datasets([ad_dataset[\"train\"], cn_dataset[\"train\"]])\n",
    "dataset = DatasetDict({\"train\": dataset})\n",
    "dataset[\"train\"], dataset[\"valid\"] = dataset[\"train\"].train_test_split(0.25).values()\n",
    "dataset = dataset.map(preprocess, remove_columns=\"audio\", batched=True)\n",
    "\n",
    "train_dataset = dataset[\"train\"].with_format(\"torch\")\n",
    "val_dataset = dataset[\"valid\"].with_format(\"torch\")\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "specificity = evaluate.load(\"nevikw39/specificity\")\n",
    "\n",
    "# Define training arguments (for evaluation purposes)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/whisper-large-v3_ADReSSo\" + (\"_fp16\" if False else \"\"),\n",
    "    fp16=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=lambda eval_pred: (\n",
    "        accuracy.compute(\n",
    "            predictions=(pred := np.argmax(eval_pred.predictions, axis=1)),\n",
    "            references=eval_pred.label_ids,\n",
    "        ) | f1.compute(\n",
    "            predictions=pred,\n",
    "            references=eval_pred.label_ids,\n",
    "        ) | specificity.compute(\n",
    "            predictions=pred,\n",
    "            references=eval_pred.label_ids,\n",
    "        )\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(10)],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9324af9a2c74ee98e7db53c2a6c5b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.5098632574081421, 'eval_model_preparation_time': 0.003, 'eval_accuracy': 0.8809523809523809, 'eval_f1': 0.8837209302325582, 'eval_specificity': 0.8571428571428571, 'eval_runtime': 6.1381, 'eval_samples_per_second': 6.842, 'eval_steps_per_second': 3.421}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the validation dataset\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation results:\", eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
